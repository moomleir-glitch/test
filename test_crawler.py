import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

# Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø´Ø±ÙˆØ¹ (Seed)
SEED_SITES = [
    "https://fa.wikipedia.org",
    "https://www.isna.ir",
    "https://www.mehrnews.com",
]

visited = set()
found_links = set()

def crawl(url, max_links=20):
    print(f"\nğŸŒ Crawling: {url}")

    try:
        r = requests.get(url, timeout=10, headers={
            "User-Agent": "MoomleBot/0.1"
        })
    except Exception as e:
        print("âŒ Ø®Ø·Ø§:", e)
        return

    soup = BeautifulSoup(r.text, "lxml")

    for a in soup.find_all("a", href=True):
        link = a["href"]

        # ØªØ¨Ø¯ÛŒÙ„ Ù„ÛŒÙ†Ú© Ù†Ø³Ø¨ÛŒ Ø¨Ù‡ Ú©Ø§Ù…Ù„
        link = urljoin(url, link)

        # ÙÙ‚Ø· http / https
        if not link.startswith("http"):
            continue

        # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒ
        if link in visited:
            continue

        # Ø­Ø°Ù ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
        if any(link.lower().endswith(ext) for ext in [".jpg",".png",".pdf",".zip"]):
            continue

        visited.add(link)
        found_links.add(link)

        print("ğŸ”—", link)

        if len(found_links) >= max_links:
            break


if __name__ == "__main__":
    for site in SEED_SITES:
        crawl(site)

    print("\nâœ… Ù¾Ø§ÛŒØ§Ù† ØªØ³Øª")
    print(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ù„ÛŒÙ†Ú© Ú©Ø´Ùâ€ŒØ´Ø¯Ù‡: {len(found_links)}")
